import streamlit as st
import torch
import openai
from transformers import BertTokenizer, BertModel
import torch.nn as nn

# Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ø¬Ù‡Ø© Streamlit Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
st.set_page_config(page_title="ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª", layout="centered")
st.title("ğŸ” Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ")

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ÙØªØ§Ø­ OpenAI Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ (ÙŠØ±Ø¬Ù‰ Ø¹Ø¯Ù… Ù…Ø´Ø§Ø±ÙƒØªÙ‡ Ù…Ø¹ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†)
openai.api_key = "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"

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø¯Ù‚Ù‚
@st.cache_resource
def load_model():
    class ReviewClassifier(nn.Module):
        def __init__(self, hidden_size=768, num_classes=6):
            super(ReviewClassifier, self).__init__()
            self.bert = BertModel.from_pretrained("bert-base-uncased")
            self.lstm = nn.LSTM(input_size=768, hidden_size=128, batch_first=True, bidirectional=True)
            self.conv1 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)
            self.relu = nn.ReLU()
            self.dropout = nn.Dropout(0.3)
            self.fc = nn.Linear(128, num_classes)

        def forward(self, input_ids, attention_mask):
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            sequence_output = outputs.last_hidden_state
            lstm_output, _ = self.lstm(sequence_output)
            lstm_output = lstm_output.permute(0, 2, 1)
            conv_output = self.relu(self.conv1(lstm_output))
            pooled_output = torch.mean(conv_output, dim=2)
            out = self.dropout(pooled_output)
            return self.fc(out)

    model = ReviewClassifier()
    model.load_state_dict(torch.load("phase1_model.pth", map_location=torch.device("cpu")))
    model.eval()
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    return model, tokenizer

model, tokenizer = load_model()

label_map = {
    0: "Satisfaction - Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
    1: "Satisfaction - Ø³Ù„Ø¨ÙŠ",
    2: "Completeness - Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
    3: "Completeness - Ø³Ù„Ø¨ÙŠ",
    4: "Correctness - Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
    5: "Correctness - Ø³Ù„Ø¨ÙŠ"
}

def classify_review(text):
    tokens = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    with torch.no_grad():
        outputs = model(tokens["input_ids"], tokens["attention_mask"])
        prediction = torch.argmax(outputs, dim=1).item()
    return label_map[prediction], prediction

def analyze_negativity_with_gpt(text):
    prompt = f"""
Ø£Ø¹Ø·Ù†ÙŠ ØªØ­Ù„ÙŠÙ„Ù‹Ø§ Ù†Ù‚Ø¯ÙŠÙ‹Ø§ Ù„Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© Ù„Ù„Ø§Ù†Ø·Ø¨Ø§Ø¹ Ø§Ù„Ø³Ù„Ø¨ÙŠØŒ Ø«Ù… Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø¹Ù…Ù„ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¬Ø±Ø¨Ø©. Ø§Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø´ÙƒÙ„ Ù†Ù‚Ø§Ø·.

Ø§Ù„Ù†Øµ:
{text}

Ø§Ù„Ù†ØªÙŠØ¬Ø©:
- Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨:
1.
- Ø§Ù„ØªÙˆØµÙŠØ§Øª:
1.
"""
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    return response.choices[0].message.content.strip()

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
review_input = st.text_area("ğŸ“ Ø£Ø¯Ø®Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù‡Ù†Ø§:", height=200)

if st.button("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"):
    if not review_input.strip():
        st.warning("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø£ÙˆÙ„Ø§Ù‹.")
    else:
        label_text, label_id = classify_review(review_input)
        st.success(f"ğŸ“Œ Ø§Ù„ØªØµÙ†ÙŠÙ: {label_text}")

        if "Ø³Ù„Ø¨ÙŠ" in label_text:
            with st.spinner("ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT..."):
                result = analyze_negativity_with_gpt(review_input)
                st.markdown(result)
        else:
            st.info("âœ… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¥ÙŠØ¬Ø§Ø¨ÙŠØŒ Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠ.")